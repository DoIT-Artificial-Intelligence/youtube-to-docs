{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"youtube-to-docs","text":"<p>Convert YouTube videos to docs/sheets for discoverability.</p> <p>See the CLI by running:</p> <pre><code>uvx youtube-to-docs --help\n</code></pre> <p>Created with the help of AI. All artifacts have been checked and work as expected.</p>"},{"location":"development/","title":"Development Guide","text":""},{"location":"development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.14 or higher</li> <li><code>uv</code></li> </ul>"},{"location":"development/#installation","title":"Installation","text":"<ol> <li>Install dependencies:    <pre><code>uv sync\n</code></pre></li> </ol>"},{"location":"development/#usage","title":"Usage","text":"<p>To run the script locally using the entry point defined in <code>pyproject.toml</code>:</p> <pre><code>uv run youtube-to-docs --model gemini-3-flash-preview\n</code></pre> <p>Alternatively, you can run it as a module:</p> <pre><code>uv run python -m youtube_to_docs.main --model gemini-3-flash-preview\n</code></pre>"},{"location":"development/#running-tests","title":"Running Tests","text":"<p>We use <code>pytest</code> for testing.</p>"},{"location":"development/#using-uv-recommended","title":"Using <code>uv</code> (Recommended)","text":"<p>To run tests with all dependencies automatically handled:</p> <pre><code>uv run --group test pytest\n</code></pre>"},{"location":"development/#utilities","title":"Utilities","text":""},{"location":"development/#cleanup","title":"Cleanup","text":"<p>To delete all generated artifacts:</p> <p>Bash: <pre><code>rm -rf youtube-to-docs-artifacts/\n</code></pre></p> <p>PowerShell: <pre><code>Remove-Item -Path \"youtube-to-docs-artifacts\" -Recurse -Force -ErrorAction SilentlyContinue\n</code></pre></p>"},{"location":"development/#project-structure","title":"Project Structure","text":"<ul> <li><code>main.py</code>: Main application script.</li> <li><code>tests/</code>: Directory containing test files.</li> <li><code>pyproject.toml</code>: Project configuration and dependencies.</li> </ul>"},{"location":"development/#tooling","title":"Tooling","text":"<p>This project uses modern Python tooling for code quality:</p> <ul> <li>Ruff: For linting and code formatting.</li> <li>Ty: For static type checking.</li> </ul> <p>These tools are configured in <code>pyproject.toml</code>.</p>"},{"location":"development/#running-ruff","title":"Running Ruff","text":"<p>To check for linting errors:</p> <pre><code>uv tool run ruff check .\n</code></pre> <p>To fix fixable linting errors automatically:</p> <pre><code>uv tool run ruff check --fix .\n</code></pre> <p>To format the code:</p> <pre><code>uv tool run ruff format .\n</code></pre>"},{"location":"development/#running-ty-type-checking","title":"Running Ty (Type Checking)","text":"<p>To run type checks:</p> <pre><code>uv tool run ty check\n</code></pre>"},{"location":"development/#pre-commit","title":"Pre-commit","text":"<p>To run pre-commit hook:</p> <pre><code>uv tool run pre-commit run --all-files\n</code></pre>"},{"location":"development/#documentation","title":"Documentation","text":"<p>We use <code>MkDocs</code> with the <code>Material</code> theme for documentation.</p>"},{"location":"development/#build-and-serve-locally","title":"Build and Serve Locally","text":"<p>To preview the documentation locally:</p> <pre><code>uv run mkdocs serve\n</code></pre> <p>This will start a local server, usually at <code>http://127.0.0.1:8000</code>.</p> <p>To build the static site (output will be in the <code>site/</code> directory):</p> <pre><code>uv run mkdocs build\n</code></pre>"},{"location":"development/#deployment","title":"Deployment","text":"<p>The documentation is automatically built and deployed to GitHub Pages on every push to the <code>main</code> branch via GitHub Actions.</p> <p>If you need to deploy manually:</p> <pre><code>uv run mkdocs gh-deploy\n</code></pre>"},{"location":"development/#release-to-pypi","title":"Release to PyPI","text":"<p>To publish a new version of the package to PyPI, follow these steps:</p> <ol> <li> <p>Build the package:     This will create a <code>dist/</code> directory with the distribution files.     <pre><code>uv tool run --from build pyproject-build\n</code></pre></p> </li> <li> <p>Upload to PyPI:     Use <code>twine</code> to upload the distribution files. </p> <p>If your <code>.pypirc</code> is already configured with your API key: <pre><code>uv tool run twine upload dist/*\n</code></pre></p> </li> </ol>"},{"location":"development/#quick-deploy-one-liner","title":"Quick Deploy (One-liner)","text":"<p>To build and deploy in one command (requires <code>.pypirc</code> configuration): <pre><code>Remove-Item -Recurse -Force dist; uv tool run --from build pyproject-build; uv tool run twine upload dist/*\n</code></pre></p> <pre><code>rm -rf dist; uv tool run --from build pyproject-build; uv tool run twine upload dist/*\n</code></pre>"},{"location":"development/#continuous-deployment","title":"Continuous Deployment","text":"<p>A manual GitHub Action is available to automate this process. </p> <ol> <li>Ensure you have a <code>PYPI_API_TOKEN</code> secret configured in your repository settings.</li> <li>Go to the \"Actions\" tab in your repository.</li> <li>Select the \"PyPI Publish\" workflow.</li> <li>Click \"Run workflow\".</li> </ol>"},{"location":"how-this-works/","title":"How This Works","text":"<p><code>youtube-to-docs</code> automates the conversion of YouTube videos into structured documentation and multimodal assets. This document explains the technical workflow and core components of the system.</p>"},{"location":"how-this-works/#system-architecture","title":"System Architecture","text":"<p>The tool operates as a pipeline that ingests YouTube content, processes it through various AI models, and outputs structured data and files.</p>"},{"location":"how-this-works/#1-input-resolution","title":"1. Input Resolution","text":"<p>The entry point (<code>youtube_to_docs/main.py</code>) accepts flexible inputs: - Video IDs: Direct processing. - Playlists: Resolves all video IDs within a playlist. - Channels: Resolves a channel's \"Uploads\" playlist to process all their videos.</p> <p>Key Component: <code>youtube_to_docs.transcript.resolve_video_ids</code> uses the YouTube Data API to fetch lists of videos when a Playlist or Channel is provided.</p>"},{"location":"how-this-works/#2-metadata-audio-transcript-retrieval","title":"2. Metadata, Audio &amp; Transcript Retrieval","text":"<p>Once video IDs are resolved, the system gathers essential data:</p> <ul> <li>Metadata: Title, description, tags, publication date, and duration are fetched via the YouTube Data API.</li> <li>Audio Extraction: If an AI model is requested for transcription (via the <code>-t</code> flag), <code>yt-dlp</code> and <code>ffmpeg</code> are used to extract high-quality audio (<code>.m4a</code>) from the video.</li> <li>Transcripts:<ul> <li>YouTube Source: By default, <code>youtube-transcript-api</code> fetches existing captions, prioritizing human-generated ones.</li> <li>AI Source: If specified, an AI model (like Gemini 2.0 Flash) processes the extracted audio file to generate a fresh, potentially higher-accuracy transcript.</li> </ul> </li> </ul> <p>Note on Auto-Captions: Automatic captions are generated by speech recognition and may have accuracy issues with accents or background noise. They are not always immediately available after upload.</p>"},{"location":"how-this-works/#3-the-llm-pipeline","title":"3. The LLM Pipeline","text":"<p>Text processing is handled by Large Language Models (LLMs) defined in <code>youtube_to_docs/llms.py</code>. The pipeline is model-agnostic, supporting Google Gemini, Vertex AI, AWS Bedrock, and Azure Foundry.</p> <p>For each video, the specified model performs three distinct tasks:</p> <ol> <li> <p>Speaker Extraction:</p> <ul> <li>Input: Full transcript.</li> <li>Task: Identify speakers and their professional titles/roles.</li> <li>Output: A structured list (e.g., \"Speaker 1 (Host)\").</li> </ul> </li> <li> <p>Q&amp;A Generation:</p> <ul> <li>Input: Full transcript + Identified Speakers.</li> <li>Task: Extract key questions and answers discussed in the video.</li> <li>Output: A Markdown table with columns for Questioner, Question, Responder, and Answer.</li> </ul> </li> <li> <p>Summarization:</p> <ul> <li>Input: Full transcript + Video Metadata.</li> <li>Task: Create a concise, comprehensive summary of the content.</li> <li>Output: A Markdown-formatted summary.</li> </ul> </li> <li> <p>Multi-Language Support:</p> <ul> <li>The tool supports processing videos in multiple languages via the <code>--language</code> argument.</li> <li>It iterates through each requested language, fetching or generating transcripts, summaries, Q&amp;A, and infographics for that specific language.</li> <li>File names and column headers are suffixed with the language code (e.g., <code>(es)</code>) to keep assets organized.</li> </ul> </li> </ol>"},{"location":"how-this-works/#5-multimodal-generation","title":"5. Multimodal Generation","text":"<p>Beyond text, the tool creates audio and visual assets:</p> <ul> <li> <p>Text-to-Speech (TTS):</p> <ul> <li>Uses models (like Gemini's TTS) to convert the generated summary into an audio file.</li> <li>This allows users to \"listen\" to the video summary.</li> </ul> </li> <li> <p>Infographics:</p> <ul> <li>Uses image generation models to create a visual representation of the summary.</li> <li>Supported Providers:<ul> <li>Google: Gemini, Imagen.</li> <li>AWS Bedrock: Titan Image Generator, Nova Canvas (requires <code>AWS_BEARER_TOKEN_BEDROCK</code>).</li> <li>Azure Foundry: GPT Image models (requires <code>AZURE_FOUNDRY_ENDPOINT</code> and <code>AZURE_FOUNDRY_API_KEY</code>).</li> </ul> </li> <li>The prompt includes the video title and the generated summary text to ensure relevance.</li> </ul> </li> </ul>"},{"location":"how-this-works/#6-cost-tracking","title":"6. Cost Tracking","text":"<p>The system includes a pricing engine (<code>youtube_to_docs/prices.py</code>) that tracks token usage for every API call. - It calculates costs for input and output tokens based on the specific model used. - Costs are aggregated for speaker extraction, Q&amp;A, summarization, and infographic generation. - These estimates are saved directly into the output CSV.</p>"},{"location":"how-this-works/#data-organization","title":"Data Organization","text":"<p>The final output is a structured CSV file (managed via <code>polars</code>) containing metadata, file paths, and AI outputs. Corresponding files are organized into subdirectories within a central artifacts folder:</p> <pre><code>youtube-to-docs-artifacts/\n\u251c\u2500\u2500 youtube-docs.csv              # The main data file\n\u251c\u2500\u2500 transcript-files/             # Raw text transcripts\n\u251c\u2500\u2500 audio-files/                  # Extracted audio files (for AI transcription)\n\u251c\u2500\u2500 speaker-extraction-files/     # Identified speakers lists\n\u251c\u2500\u2500 qa-files/                     # Markdown Q&amp;A tables\n\u251c\u2500\u2500 summary-files/                # Markdown summaries\n\u251c\u2500\u2500 infographic-files/            # Generated infographic images\n\u2514\u2500\u2500 video-files/                  # Combined infographic + audio videos\n</code></pre> <p>This structure ensures that while the CSV provides a high-level data view, the actual content is easily accessible as standalone files.</p>"},{"location":"usage/","title":"Usage Guide","text":"<p><code>youtube-to-docs</code> is a versatile tool designed to convert YouTube content into structured documentation, including transcripts, summaries, audio, and infographics. It is primarily designed as a Command Line Interface (CLI) tool but can also be used as a Python library.</p>"},{"location":"usage/#command-line-interface-cli","title":"Command Line Interface (CLI)","text":"<p>The main command is <code>youtube-to-docs</code>.</p>"},{"location":"usage/#basic-usage","title":"Basic Usage","text":"<p>Running the command without arguments processes a default video:</p> <pre><code>youtube-to-docs\n</code></pre>"},{"location":"usage/#arguments","title":"Arguments","text":"Argument Description Default Example <code>video_id</code> The YouTube content to process. Can be a Video ID, Playlist ID (starts with <code>PL</code>), Channel Handle (starts with <code>@</code>), or a comma-separated list of Video IDs. <code>atmGAHYpf_c</code> <code>youtube-to-docs @mychannel</code> <code>-o</code>, <code>--outfile</code> Path to save the output CSV file. <code>youtube-to-docs-artifacts/youtube-docs.csv</code> <code>-o my-data.csv</code> <code>-t</code>, <code>--transcript</code> The transcript source to use. Can be <code>'youtube'</code> (default) to fetch existing YouTube transcripts, or an AI model name to perform STT on extracted audio. <code>youtube</code> <code>-t gemini-2.0-flash-exp</code> <code>-m</code>, <code>--model</code> The LLM(s) to use for speaker extraction, Q&amp;A generation, and summarization. Supports models from Google (Gemini), Vertex AI, AWS Bedrock, and Azure Foundry. Can be a comma-separated list. <code>None</code> <code>-m gemini-3-flash-preview,vertex-claude-haiku-4-5@20251001</code> <code>--tts</code> The TTS model and voice to use for generating audio summaries. Format: <code>{model}-{voice}</code>. <code>None</code> <code>--tts gemini-2.5-flash-preview-tts-Kore</code> <code>--infographic</code> The image model to use for generating a visual summary. Supports models from Google (Gemini, Imagen), AWS Bedrock (Titan, Nova Canvas), and Azure Foundry. <code>None</code> <code>--infographic gemini-2.5-flash-image</code> <code>--no-youtube-summary</code> If set, skips generating a secondary summary from the YouTube transcript when using an AI model for the primary transcript. <code>False</code> <code>--no-youtube-summary</code> <code>-l</code>, <code>--language</code> The target language(s) (e.g. 'es', 'fr', 'en'). Can be a comma-separated list. Default is 'en'. <code>en</code> <code>-l es,fr</code>"},{"location":"usage/#examples","title":"Examples","text":"<p>1. Summarize the default video using a single model: <pre><code>youtube-to-docs -m gemini-3-flash-preview\n</code></pre></p> <p>2. Generate a transcript using Gemini 2.0 Flash and summarize: <pre><code>youtube-to-docs -t gemini-2.0-flash-exp -m gemini-3-flash-preview\n</code></pre></p> <p>3. Process the default video and save to a custom CSV: <pre><code>youtube-to-docs -o my-docs.csv\n</code></pre></p> <p>4. Summarize a Playlist using multiple models (Gemini and Vertex): <pre><code>youtube-to-docs PLGKTTEqwhiHHWO-jdxM1KtzTbWo6h0Ycl -m gemini-3-flash-preview,vertex-claude-haiku-4-5@20251001\n</code></pre></p> <p>5. Process a Channel with Summaries, TTS, and Infographics: <pre><code>youtube-to-docs @mga-othercommittees6625 -m vertex-claude-haiku-4-5@20251001 --tts gemini-2.5-flash-preview-tts-Kore --infographic gemini-2.5-flash-image\n</code></pre></p> <p>6. Generate an Infographic using AWS Bedrock: <pre><code>youtube-to-docs atmGAHYpf_c --infographic bedrock-titan-image-generator-v2:0\n</code></pre></p>"},{"location":"usage/#csv-column-reference","title":"CSV Column Reference","text":"<p>The output CSV file contains a variety of columns depending on the arguments provided. Below is a reference of the possible columns:</p>"},{"location":"usage/#base-metadata","title":"Base Metadata","text":"<ul> <li>URL: The full YouTube video URL.</li> <li>Title: The title of the video.</li> <li>Description: The video description.</li> <li>Data Published: The date the video was published.</li> <li>Channel: The name of the YouTube channel.</li> <li>Tags: Video tags (comma-separated).</li> <li>Duration: The duration of the video.</li> <li>Transcript characters from youtube: The total number of characters in the YouTube transcript.</li> <li>Transcript characters from {model}: The total number of characters in the AI-generated transcript (if applicable).</li> <li>Audio File: Path to the extracted audio file (used for AI transcription).</li> </ul>"},{"location":"usage/#files","title":"Files","text":"<ul> <li>Transcript File {type}: Path to the saved transcript file. <code>{type}</code> is either <code>youtube generated</code>, <code>human generated</code>, or <code>{model} generated</code>.</li> <li>Speakers File {model}: Path to the saved speaker extraction text file.</li> <li>QA File {model}: Path to the saved Q&amp;A Markdown file.</li> <li>Summary File {model}: Path to the Markdown summary file generated by a specific model.</li> <li>Summary Infographic File {model} {infographic_model}: Path to the generated infographic image.</li> <li>Summary Audio File {model} {tts_model} File: Path to the generated TTS audio file.</li> </ul>"},{"location":"usage/#ai-outputs-costs","title":"AI Outputs &amp; Costs","text":"<ul> <li>Speakers {model}: The extracted list of speakers and their roles.</li> <li>{normalized_model} Speaker extraction cost ($): The estimated API cost for speaker extraction.</li> <li>QA Text {model}: The full text of the Q&amp;A pairs (also saved to the Q&amp;A file).</li> <li>{normalized_model} QA cost ($): The estimated API cost for Q&amp;A generation.</li> <li>Summary Text {model}: The full text of the summary (also saved to the summary file).</li> <li>{normalized_model} summary cost ($): The total estimated API cost for both speaker extraction and summarization.</li> <li>Summary Infographic Cost {model} {infographic_model} ($): The estimated API cost for infographic generation.</li> <li>{normalized_model} STT cost ($): The estimated API cost for Speech-to-Text generation.</li> </ul> <p>Note: <code>{normalized_model}</code> refers to the model name with prefixes (like <code>vertex-</code>) and date suffixes removed for cleaner column headers.</p>"},{"location":"usage/#speaker-extraction","title":"Speaker Extraction","text":"<p>When a model is specified using the <code>-m</code> or <code>--model</code> argument, the tool automatically performs speaker extraction before generating a summary.</p> <ul> <li>Model Matching: The extraction uses the same model as the summary. If multiple models are provided, each will perform its own extraction.</li> <li>Structured Output: It identifies speakers and their professional titles or roles (e.g., \"Speaker 1 (Senator Katie Fry Hester, Co-Chair)\").</li> <li>Cost Tracking: The cost of speaker extraction is tracked separately in the <code>{model} Speaker extraction cost ($)</code> column and included in the total <code>{model} summary cost ($)</code>.</li> <li>Unknowns: If a speaker or title cannot be identified, the tool uses the placeholder <code>UNKNOWN</code>. If no speakers are detected at all, the field is set to <code>NaN</code>.</li> </ul>"},{"location":"usage/#mcp-server","title":"MCP Server","text":"<p>This tool also functions as a Model Context Protocol (MCP) server, allowing it to be used as a tool by AI agents (like the Gemini CLI).</p> <p>The server exposes a <code>process_video</code> tool that mirrors the CLI functionality.</p>"},{"location":"usage/#configuration","title":"Configuration","text":"<p>The repository includes a <code>gemini-extension.json</code> file at the root, which configures the MCP server for use with the Gemini CLI.</p>"},{"location":"usage/#usage","title":"Usage","text":"<p>Once the extension is registered with your agent, you can ask it to process videos using natural language:</p> <p>\"Save a summary of https://www.youtube.com/watch?v=KuPc06JgI_A\"</p> <p>The agent will prompt you for any necessary details (like the model to use) and then execute the tool.</p>"},{"location":"usage/#library-usage","title":"Library Usage","text":"<p>While primarily a CLI, you can import core functions for custom workflows.</p> <pre><code>from youtube_to_docs.transcript import fetch_transcript\n\nvideo_id = \"atmGAHYpf_c\"\ntranscript, is_generated = fetch_transcript(video_id)\n</code></pre>"},{"location":"usage/#why-use-youtube-to-docs","title":"Why use <code>youtube-to-docs</code>?","text":"<p>You might find other tools that download YouTube transcripts, but <code>youtube-to-docs</code> distinguishes itself in several ways:</p> <ol> <li> <p>Multimodal Output: It doesn't just stop at text.</p> <ul> <li>Summaries: Uses state-of-the-art LLMs to create concise summaries.</li> <li>Speaker Extraction: Automatically identifies speakers and their titles/roles from the transcript.</li> <li>Audio (TTS): Converts summaries into audio files, perfect for listening on the go.</li> <li>Visuals (Infographics): Generates AI-created infographics to visually represent the content.</li> </ul> </li> <li> <p>Structured Data (CSV/Polars):</p> <ul> <li>Instead of loose files, metadata and paths are organized into a robust CSV file using <code>polars</code>.</li> <li>This makes it incredibly easy to import the data into Google Sheets, Excel, or a database for further analysis or publishing.</li> </ul> </li> <li> <p>Batch Processing:</p> <ul> <li>Seamlessly handles individual videos, entire playlists, or full channels with a single command.</li> </ul> </li> <li> <p>Multi-Provider Support:</p> <ul> <li>Agnostic to the LLM provider. Whether you use Google Gemini, Vertex AI, AWS Bedrock, or Azure Foundry, you can plug in your preferred model.</li> </ul> </li> <li> <p>Cost Awareness:</p> <ul> <li>When using paid API models, it tracks and estimates the cost of summarization, saving it directly to your data file.</li> </ul> </li> </ol>"}]}